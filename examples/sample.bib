@inproceedings{bentivogli2009fifth,
  author = {Bentivogli, Luisa et. al},
  booktitle = {Proceedings of the Text Analysis Conference (TAC)},
  title = {{The Fifth PASCAL Recognizing Textual Entailment Challenge}},
  year = {2009}
}

@inproceedings{chen2022taskspecific,
  author = {Chen, Meng and others},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  title = {{Task-specific Expert Pruning for Mixture-of-Expert Models}},
  year = {2022}
}

@inproceedings{lee2024stun,
  author = {Lee, Jaeseong et. al},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  doi = {10.48550/arXiv.2409.06211},
  title = {Stun: Structured-then-unstructured pruning for scalable moe pruning},
  year = {2024}
}

@misc{qwen_moe15,
  author = {Qwen Team},
  month = {February},
  title = {Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters"},
  url = {https://qwenlm.github.io/blog/qwen-moe/},
  year = {2024}
}

@article{liu2024efficient,
  author = {Liu, Enshu et. al},
  doi = {10.48550/arXiv.2407.00945},
  journal = {arXiv.org},
  title = {Efficient expert pruning for sparse mixture-of-experts language models: Enhancing performance and reducing inference costs},
  year = {2024}
}

@article{fedus2022switch,
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal = {Journal of Machine Learning Research},
  number = {120},
  pages = {1--39},
  title = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  volume = {23},
  year = {2022}
}

@inproceedings{chen2025retrainingfreemergingsparsemoe,
  author = {I-Chun Chen et. al},
  booktitle = {International Conference on Machine Learning},
  title = {Retraining-Free Merging of Sparse MoE via Hierarchical Clustering},
  url = {https://arxiv.org/abs/2410.08589},
  year = {2024}
}

@misc{clark2018think,
  archiveprefix = {arXiv},
  author = {Clark, Peter and others},
  eprint = {1803.05457},
  title = {{Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}},
  year = {2018}
}

@inproceedings{clark2019boolq,
  author = {Clark, Christopher and others},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title = {{BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}},
  year = {2019}
}

@inproceedings{frantar2023sparsegpt,
  author = {Frantar, Elias and Alistarh, Dan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  title = {{SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot}},
  year = {2023}
}

@misc{eval-harness,
  author = {Gao, Leo and others},
  doi = {10.5281/zenodo.5371628},
  publisher = {Zenodo},
  title = {{A framework for few-shot language model evaluation}},
  year = {2021}
}

@inproceedings{hendrycks2021measuring,
  author = {Hendrycks, Dan and others},
  booktitle = {Proceedings of the 9th International Conference on Learning Representations (ICLR)},
  title = {{Measuring Massive Multitask Language Understanding}},
  year = {2021}
}

@article{hoefler2021sparsity,
  author = {Hoefler, Torsten and others},
  journal = {Journal of Machine Learning Research},
  number = {241},
  pages = {1--124},
  title = {{Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks}},
  volume = {22},
  year = {2021}
}

@article{jiang2024mixtral,
  author = {Jiang, Albert Q. and others},
  doi = {10.48550/arXiv.2401.04088},
  journal = {arXiv.org},
  title = {{Mixtral of Experts}},
  year = {2024}
}

@misc{kaddour2023challenges,
  archiveprefix = {arXiv},
  author = {Kaddour, Jean and others},
  eprint = {2307.10169},
  title = {{Challenges and Applications of Large Language Models}},
  year = {2023}
}

@misc{kaplan2020scaling,
  archiveprefix = {arXiv},
  author = {Kaplan, Jared and others},
  eprint = {2001.08361},
  title = {{Scaling Laws for Neural Language Models}},
  year = {2020}
}

@inproceedings{liu2024m,
  author = {Liu, Zhen and others},
  booktitle = {Proceedings of the 12th International Conference on Learning Representations (ICLR)},
  title = {{M-SMoE: A Memory-Efficient High-Performance MoE Implementation}},
  year = {2024}
}

@inproceedings{lu2024not,
  author = {Lu, Yilin and others},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  title = {{Not All Experts Are Equal: Redundancy-aware and Task-aware Expert Pruning for Mixture-of-Expert Models}},
  year = {2024}
}

@inproceedings{mihaylov2018can,
  author = {Mihaylov, Todor and others},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  title = {{Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}},
  year = {2018}
}

@misc{qwen2024moe,
  author = {{Qwen Team}},
  publisher = {Hugging Face},
  title = {{Qwen1.5-MoE-A2.7B}},
  url = {https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B},
  year = {2024}
}

@article{raffel2020exploring,
  author = {Raffel, Colin and others},
  journal = {Journal of Machine Learning Research},
  number = {140},
  pages = {1--67},
  title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
  volume = {21},
  year = {2020}
}

@inproceedings{rajbhandari2022deepspeed,
  author = {Rajbhandari, Samyam and others},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning (ICML)},
  title = {{DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale}},
  year = {2022}
}

@article{sakaguchi2021winogrande,
  author = {Sakaguchi, Keisuke and others},
  journal = {Communications of the ACM},
  number = {9},
  pages = {99--106},
  title = {{Winogrande: An Adversarial Winograd Schema Challenge at Scale}},
  volume = {64},
  year = {2021}
}

@misc{sarkar2024revisiting,
  archiveprefix = {arXiv},
  author = {Sarkar, Anshuman and others},
  eprint = {2401.00898},
  title = {{Revisiting Pretraining Objectives for General-Purpose Language Models}},
  year = {2024}
}

@misc{shazeer2017outrageously,
  archiveprefix = {arXiv},
  author = {Shazeer, Noam and others},
  eprint = {1701.06538},
  title = {{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}},
  year = {2017}
}

@inproceedings{sun2024wanda,
  author = {Sun, Mingjie and others},
  booktitle = {Proceedings of the 12th International Conference on Learning Representations (ICLR)},
  title = {{A Simple and Effective Pruning Approach for Large Language Models}},
  year = {2024}
}

@inproceedings{zellers2019hellaswag,
  author = {Zellers, Rowan and others},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  title = {{HellaSwag: Can a Machine Really Finish Your Sentence?}},
  year = {2019}
}

@misc{kaplan2020scaling,
  archiveprefix = {arXiv},
  author = {Jared Kaplan et. al},
  eprint = {2001.08361},
  primaryclass = {cs.LG},
  title = {Scaling Laws for Neural Language Models},
  year = {2020}
}

@misc{sun2023wanda,
  archiveprefix = {arXiv},
  author = {Zichao Ye et. al},
  eprint = {2306.11695},
  primaryclass = {cs.LG},
  title = {Wanda: A Simple and Effective Pruning Approach for Large Language Models},
  year = {2023}
}

@article{qwen2024qwen1.5,
  author = {Jinze Bai et al. },
  doi = {10.48550/arXiv.2403.04253},
  journal = {International Conference on Learning Representations},
  title = {Qwen1.5: Advancements in Large Language Models},
  year = {2024}
}

@misc{gpt4,
  archiveprefix = {arXiv},
  author = {OpenAI et. al},
  eprint = {2303.08774},
  primaryclass = {cs.CL},
  title = {{GPT-4} Technical Report},
  year = {2024}
}

@inproceedings{gao2021framework,
  author = {Leo Gao et. al},
  booktitle = {Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR52688.2022.00780},
  title = {A Framework for Few-Shot Language Model Evaluation},
  year = {2021}
}

@inproceedings{dagan2005pascal,
  author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle = {Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment},
  pages = {177--190},
  publisher = {Springer},
  title = {The {PASCAL} recognising textual entailment challenge},
  year = {2005}
}

@inproceedings{bar2006second,
  author = {Bar-Haim, Roy et. al},
  booktitle = {Proceedings of the second PASCAL challenges workshop on recognising textual entailment},
  pages = {1--4},
  title = {The second {PASCAL} recognising textual entailment challenge},
  volume = {6},
  year = {2006}
}

@inproceedings{giampiccolo2007third,
  author = {Giampiccolo, Danilo et. al},
  booktitle = {Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages = {1--9},
  title = {The third {PASCAL} recognizing textual entailment challenge},
  year = {2007}
}

@article{muzio2024seer,
  author = {Muzio, Alexandre and Sun, Alex and He, Churan},
  doi = {10.48550/arXiv.2404.05089},
  journal = {arXiv.org},
  title = {Seer-moe: Sparse expert efficiency through regularization for mixture-of-experts},
  year = {2024}
}

@article{chen2022task,
  author = {Chen, Tianyu et. al},
  doi = {10.48550/arXiv.2206.00277},
  journal = {arXiv.org},
  title = {Task-specific expert pruning for sparse mixture-of-experts},
  year = {2022}
}

@inproceedings{brown2020language,
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  address = {Red Hook, NY, USA},
  articleno = {159},
  author = {Brown, Tom B. et al.},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  isbn = {9781713829546},
  location = {Vancouver, BC, Canada},
  numpages = {25},
  publisher = {Curran Associates Inc.},
  series = {NIPS '20},
  title = {Language models are few-shot learners},
  year = {2020}
}

@inproceedings{chowdhury2024provably,
  author = {Chowdhury, Mohammed Nowaz Rabbani et. al},
  booktitle = {International Conference on Machine Learning},
  doi = {10.48550/arXiv.2405.16646},
  title = {A provably effective method for pruning experts in fine-tuned sparse mixture-of-experts},
  year = {2024}
}

@article{xie2024moe,
  author = {Xie, Yanyue et. al},
  doi = {10.48550/arXiv.2410.12013},
  journal = {arXiv.org},
  title = {Moe-pruner: Pruning mixture-of-experts large language model using the hints from its router},
  year = {2024}
}

@article{dong2025domain,
  author = {Dong, Zican et. al},
  journal = {arXiv preprint arXiv:2504.06792},
  title = {Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations},
  year = {2025}
}

@inproceedings{pal2022medmcqa,
  author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle = {Conference on health, inference, and learning},
  organization = {PMLR},
  pages = {248--260},
  title = {Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  year = {2022}
}

@article{amini2019mathqa,
  author = {Amini, Aida et. al},
  journal = {arXiv preprint arXiv:1905.13319},
  title = {Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  year = {2019}
}

@misc{he2024towards,
  archiveprefix = {arXiv},
  author = {Shwai He et. al},
  eprint = {2406.02500},
  primaryclass = {cs.LG},
  title = {Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques},
  url = {https://arxiv.org/abs/2406.02500},
  year = {2025}
}

@article{liu2024deepseek,
  author = {Liu, Aixin et. al},
  doi = {10.48550/arXiv.2405.04434},
  journal = {arXiv.org},
  title = {Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  year = {2024}
}

@article{lepikhin2020gshard,
  author = {Lepikhin, Dmitry et. al},
  journal = {arXiv preprint arXiv:2006.16668},
  title = {Gshard: Scaling giant models with conditional computation and automatic sharding},
  year = {2020}
}

@inproceedings{du2022glm,
  author = {Du, Zhengxiao et. al},
  booktitle = {Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers)},
  pages = {320--335},
  title = {Glm: General language model pretraining with autoregressive blank infilling},
  year = {2022}
}
