@inproceedings{bentivogli2009fifth,
  author    = {Bentivogli, Luisa and Dagan, Ido and Dang, Hoa Trang and Giampiccolo, Danilo and Magnini, Bernardo},
  title     = {{The Fifth PASCAL Recognizing Textual Entailment Challenge}},
  booktitle = {Proceedings of the Text Analysis Conference (TAC)},
  year      = {2009}
}

@inproceedings{chen2022taskspecific,
  author    = {Chen, Meng and others},
  title     = {{Task-specific Expert Pruning for Mixture-of-Expert Models}},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  year      = {2022}
}

@article{lee2024stun,
  title={Stun: Structured-then-unstructured pruning for scalable moe pruning},
  author={Lee, Jaeseong and Qiao, Aurick and Campos, Daniel F and Yao, Zhewei and He, Yuxiong and others},
  journal={arXiv preprint arXiv:2409.06211},
  year={2024}
}

@misc{qwen_moe15,
    title = {Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters"},
    url = {https://qwenlm.github.io/blog/qwen-moe/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}

@article{liu2024efficient,
  title={Efficient expert pruning for sparse mixture-of-experts language models: Enhancing performance and reducing inference costs},
  author={Liu, Enshu and Zhu, Junyi and Lin, Zinan and Ning, Xuefei and Blaschko, Matthew B and Yan, Shengen and Dai, Guohao and Yang, Huazhong and Wang, Yu},
  journal={arXiv preprint arXiv:2407.00945},
  year={2024}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@misc{chen2025retrainingfree,
  author        = {Chen, Zixiang and others},
  title         = {{Retraining-Free Merging for Sparse Mixture-of-Experts}},
  year          = {2024},
  eprint        = {2406.01728},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{clark2018think,
  author    = {Clark, Peter and others},
  title     = {{Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}},
  year      = {2018},
  eprint    = {1803.05457},
  archivePrefix = {arXiv}
}

@inproceedings{clark2019boolq,
  author    = {Clark, Christopher and others},
  title     = {{BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2019}
}


@inproceedings{frantar2023sparsegpt,
  author    = {Frantar, Elias and Alistarh, Dan},
  title     = {{SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot}},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  year      = {2023}
}

@misc{eval-harness,
  author       = {Gao, Leo and others},
  title        = {{A framework for few-shot language model evaluation}},
  year         = {2021},
  doi          = {10.5281/zenodo.5371628},
  publisher    = {Zenodo}
}

@inproceedings{hendrycks2021measuring,
  author    = {Hendrycks, Dan and others},
  title     = {{Measuring Massive Multitask Language Understanding}},
  booktitle = {Proceedings of the 9th International Conference on Learning Representations (ICLR)},
  year      = {2021}
}

@article{hoefler2021sparsity,
  author  = {Hoefler, Torsten and others},
  title   = {{Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks}},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {241},
  pages   = {1--124}
}

@misc{jiang2024mixtral,
  author    = {Jiang, Albert Q. and others},
  title     = {{Mixtral of Experts}},
  year      = {2024},
  eprint    = {2401.04088},
  archivePrefix = {arXiv}
}

@misc{kaddour2023challenges,
  author    = {Kaddour, Jean and others},
  title     = {{Challenges and Applications of Large Language Models}},
  year      = {2023},
  eprint    = {2307.10169},
  archivePrefix = {arXiv}
}

@misc{kaplan2020scaling,
  author    = {Kaplan, Jared and others},
  title     = {{Scaling Laws for Neural Language Models}},
  year      = {2020},
  eprint    = {2001.08361},
  archivePrefix = {arXiv}
}

@inproceedings{liu2024m,
  author    = {Liu, Zhen and others},
  title     = {{M-SMoE: A Memory-Efficient High-Performance MoE Implementation}},
  booktitle = {Proceedings of the 12th International Conference on Learning Representations (ICLR)},
  year      = {2024}
}

@inproceedings{lu2024not,
  author    = {Lu, Yilin and others},
  title     = {{Not All Experts Are Equal: Redundancy-aware and Task-aware Expert Pruning for Mixture-of-Expert Models}},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2024}
}

@inproceedings{mihaylov2018can,
  author    = {Mihaylov, Todor and others},
  title     = {{Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018}
}

@misc{qwen2024moe,
  author    = {{Qwen Team}},
  title     = {{Qwen1.5-MoE-A2.7B}},
  year      = {2024},
  publisher = {Hugging Face},
  url       = {https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B}
}

@article{raffel2020exploring,
  author  = {Raffel, Colin and others},
  title   = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67}
}

@inproceedings{rajbhandari2022deepspeed,
  author    = {Rajbhandari, Samyam and others},
  title     = {{DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale}},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning (ICML)},
  year      = {2022}
}

@article{sakaguchi2021winogrande,
  author  = {Sakaguchi, Keisuke and others},
  title   = {{Winogrande: An Adversarial Winograd Schema Challenge at Scale}},
  journal = {Communications of the ACM},
  year    = {2021},
  volume  = {64},
  number  = {9},
  pages   = {99--106}
}

@misc{sarkar2024revisiting,
  author    = {Sarkar, Anshuman and others},
  title     = {{Revisiting Pretraining Objectives for General-Purpose Language Models}},
  year      = {2024},
  eprint    = {2401.00898},
  archivePrefix = {arXiv}
}

@misc{shazeer2017outrageously,
  author    = {Shazeer, Noam and others},
  title     = {{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}},
  year      = {2017},
  eprint    = {1701.06538},
  archivePrefix = {arXiv}
}

@inproceedings{sun2024wanda,
  author    = {Sun, Mingjie and others},
  title     = {{A Simple and Effective Pruning Approach for Large Language Models}},
  booktitle = {Proceedings of the 12th International Conference on Learning Representations (ICLR)},
  year      = {2024}
}

@inproceedings{zellers2019hellaswag,
  author    = {Zellers, Rowan and others},
  title     = {{HellaSwag: Can a Machine Really Finish Your Sentence?}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2019}
}

@misc{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{sun2023wanda,
    title={Wanda: A Simple and Effective Pruning Approach for Large Language Models},
    author={Zichao Ye and Yuzheng Sun and Hong-You Chen and Zeren Chen and Pin-Yu Chen and Sheng-Yu Wang and Kai-Wei Chang and Yubei Chen},
    year={2023},
    eprint={2306.11695},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{qwen2024qwen1.5,
    title={Qwen1.5: Advancements in Large Language Models},
    author={Jinze Bai et al. },
    year={2024},
    eprint={2403.04253},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{gpt4,
      title={{GPT-4} Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and others},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{gao2021framework,
    title={A Framework for Few-Shot Language Model Evaluation},
    author={Leo Gao and Jonathan Tow and Stella Biderman and Sid Black and Anthony DiPofi and Charles Foster and Laurence Golding and Jeffrey Hsu and Kyle McDonell and Niklas Muennighoff and Jason Phang and Laria Reynolds and Eric Tang and Anish Thite and Ben Wang and Kevin Wang and Andy Zou},
    year={2021},
    eprint={2109.01903},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{dagan2005pascal,
  title={The {PASCAL} recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment},
  pages={177--190},
  year={2005},
  publisher={Springer}
}

@inproceedings{bar2006second,
  title={The second {PASCAL} recognising textual entailment challenge},
  author={Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  booktitle={Proceedings of the second PASCAL challenges workshop on recognising textual entailment},
  volume={6},
  pages={1--4},
  year={2006}
}

@inproceedings{giampiccolo2007third,
  title={The third {PASCAL} recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007}
}

@article{muzio2024seer,
  title={Seer-moe: Sparse expert efficiency through regularization for mixture-of-experts},
  author={Muzio, Alexandre and Sun, Alex and He, Churan},
  journal={arXiv preprint arXiv:2404.05089},
  year={2024}
}

@article{chen2022task,
  title={Task-specific expert pruning for sparse mixture-of-experts},
  author={Chen, Tianyu and Huang, Shaohan and Xie, Yuan and Jiao, Binxing and Jiang, Daxin and Zhou, Haoyi and Li, Jianxin and Wei, Furu},
  journal={arXiv preprint arXiv:2206.00277},
  year={2022}
}

@inproceedings{brown2020language,
author = {Brown, Tom B. et al.},
title = {Language models are few-shot learners},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {159},
numpages = {25},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{chowdhury2024provably,
  title={A provably effective method for pruning experts in fine-tuned sparse mixture-of-experts},
  author={Chowdhury, Mohammed Nowaz Rabbani and Wang, Meng and Maghraoui, Kaoutar El and Wang, Naigang and Chen, Pin-Yu and Carothers, Christopher},
  journal={arXiv preprint arXiv:2405.16646},
  year={2024}
}


@article{xie2024moe,
  title={Moe-pruner: Pruning mixture-of-experts large language model using the hints from its router},
  author={Xie, Yanyue and Zhang, Zhi and Zhou, Ding and Xie, Cong and Song, Ziang and Liu, Xin and Wang, Yanzhi and Lin, Xue and Xu, An},
  journal={arXiv preprint arXiv:2410.12013},
  year={2024}
}

@article{dong2025domain,
  title={Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations},
  author={Dong, Zican and Peng, Han and Liu, Peiyu and Zhao, Wayne Xin and Wu, Dong and Xiao, Feng and Wang, Zhifeng},
  journal={arXiv preprint arXiv:2504.06792},
  year={2025}
}


@inproceedings{pal2022medmcqa,
  title={Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle={Conference on health, inference, and learning},
  pages={248--260},
  year={2022},
  organization={PMLR}
}

@article{amini2019mathqa,
  title={Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1905.13319},
  year={2019}
}


@misc{he2024towards,
      title={Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques}, 
      author={Shwai He and Daize Dong and Liang Ding and Ang Li},
      year={2025},
      eprint={2406.02500},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.02500}, 
}
@article{liu2024deepseek,
  title={Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}




@inproceedings{du2022glm,
  title={Glm: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers)},
  pages={320--335},
  year={2022}
}